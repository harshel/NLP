{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOOK NAME : Bhargav-Srinivasa-Desikan-_Bhargav-Srinivasa-Desikan_-Natural-Language-Processing-and-Computational"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USING GENSIM\n",
    "#### FOR CREATING BOW AND TFIDF MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [u\"Football club Arsenal defeat local rivals this weekend.\",\n",
    "             u\"Weekend football frenzy takes over London.\", \n",
    "             u\"Bank open for takeover bids after losing millions.\", u\"London football clubs bid to move to Wembley stadium.\", \n",
    "             u\"Arsenal bid 50 million pounds for striker Kane.\", \n",
    "             u\"Financial troubles result in loss of millions for bank.\", \n",
    "             u\"Western bank files for bankruptcy after financial losses.\", \n",
    "             u\"London football club is taken over by oil millionaire from Russia.\", \n",
    "             u\"Banking on finances not working for Russia.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is our corpus (which is basically the collection of all our documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the required libraries\n",
    "import spacy  #for preprocessing and cleaning\n",
    "from gensim import corpora #for creating our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the english language\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "texts = []\n",
    "for documnet in documents:\n",
    "    doc = nlp(documnet)\n",
    "    text = []\n",
    "    for words in doc:\n",
    "        if not words.is_stop and not words.is_punct and not words.like_num:\n",
    "            text.append(words.lemma_)\n",
    "    texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['football', 'club', 'arsenal', 'defeat', 'local', 'rival', 'weekend'],\n",
       " ['weekend', 'football', 'frenzy', 'take', 'london'],\n",
       " ['bank', 'open', 'takeover', 'bid', 'lose', 'million'],\n",
       " ['london', 'football', 'club', 'bid', 'wembley', 'stadium'],\n",
       " ['arsenal', 'bid', 'pound', 'striker', 'kane'],\n",
       " ['financial', 'trouble', 'result', 'loss', 'million', 'bank'],\n",
       " ['western', 'bank', 'file', 'bankruptcy', 'financial', 'loss'],\n",
       " ['london', 'football', 'club', 'take', 'oil', 'millionaire', 'russia'],\n",
       " ['bank', 'finance', 'work', 'russia']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts #gives the cleaned corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'arsenal': 0, 'club': 1, 'defeat': 2, 'football': 3, 'local': 4, 'rival': 5, 'weekend': 6, 'frenzy': 7, 'london': 8, 'take': 9, 'bank': 10, 'bid': 11, 'lose': 12, 'million': 13, 'open': 14, 'takeover': 15, 'stadium': 16, 'wembley': 17, 'kane': 18, 'pound': 19, 'striker': 20, 'financial': 21, 'loss': 22, 'result': 23, 'trouble': 24, 'bankruptcy': 25, 'file': 26, 'western': 27, 'millionaire': 28, 'oil': 29, 'russia': 30, 'finance': 31, 'work': 32}\n"
     ]
    }
   ],
   "source": [
    "#next we will be obtianing a bow model using gensim\n",
    "from gensim import corpora\n",
    "dictionery = corpora.Dictionary(texts)\n",
    "print(dictionery.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE : Remember the type of input the corpora.Dictionery() expects to obtain the BOW model. The input given here is in the form of list of lists where each list contain the tokenized form of the corresponding document.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)],\n",
       " [(3, 1), (6, 1), (7, 1), (8, 1), (9, 1)],\n",
       " [(10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1)],\n",
       " [(1, 1), (3, 1), (8, 1), (11, 1), (16, 1), (17, 1)],\n",
       " [(0, 1), (11, 1), (18, 1), (19, 1), (20, 1)],\n",
       " [(10, 1), (13, 1), (21, 1), (22, 1), (23, 1), (24, 1)],\n",
       " [(10, 1), (21, 1), (22, 1), (25, 1), (26, 1), (27, 1)],\n",
       " [(1, 1), (3, 1), (8, 1), (9, 1), (28, 1), (29, 1), (30, 1)],\n",
       " [(10, 1), (30, 1), (31, 1), (32, 1)]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#once the obtain the BOW model we can also represent each documnet in our corpus in the form of integer\n",
    "corpus = [dictionery.doc2bow(text) for text in texts]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here you can see that each document is represented as the BOW model. Here the tuples for each tokens represent (word_id, word_count)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.3292179861221233), (1, 0.24046829370585296), (2, 0.4809365874117059), (3, 0.1774993848325406), (4, 0.4809365874117059), (5, 0.4809365874117059), (6, 0.3292179861221233)]\n",
      "[(3, 0.24212967666975266), (6, 0.4490913847888623), (7, 0.6560530929079719), (8, 0.32802654645398593), (9, 0.4490913847888623)]\n",
      "[(10, 0.18797844084016113), (11, 0.25466485399352906), (12, 0.5093297079870581), (13, 0.3486540744136096), (14, 0.5093297079870581), (15, 0.5093297079870581)]\n",
      "[(1, 0.29431054749542984), (3, 0.21724253258131512), (8, 0.29431054749542984), (11, 0.29431054749542984), (16, 0.5886210949908597), (17, 0.5886210949908597)]\n",
      "[(0, 0.354982288765831), (11, 0.25928712547209604), (18, 0.5185742509441921), (19, 0.5185742509441921), (20, 0.5185742509441921)]\n",
      "[(10, 0.19610384738673725), (13, 0.3637247180792822), (21, 0.3637247180792822), (22, 0.3637247180792822), (23, 0.5313455887718271), (24, 0.5313455887718271)]\n",
      "[(10, 0.18286519950508276), (21, 0.3391702611796705), (22, 0.3391702611796705), (25, 0.4954753228542582), (26, 0.4954753228542582), (27, 0.4954753228542582)]\n",
      "[(1, 0.2645025265769199), (3, 0.1952400253294319), (8, 0.2645025265769199), (9, 0.3621225392416359), (28, 0.5290050531538398), (29, 0.5290050531538398), (30, 0.3621225392416359)]\n",
      "[(10, 0.22867660961662029), (30, 0.4241392327204109), (31, 0.6196018558242014), (32, 0.6196018558242014)]\n"
     ]
    }
   ],
   "source": [
    "#we can also easily obtain a TFIDF model using gensim\n",
    "from gensim.models import tfidfmodel\n",
    "tfidf = tfidfmodel.TfidfModel(corpus)\n",
    "\n",
    "#to see our tfidf corpus\n",
    "for doc in tfidf[corpus]:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here as well the tuples for each token represent ----> (word_id, tfidf_score)**\n",
    "\n",
    "**NOTE : Again remember the input that the tfidfmodel.TfidfModel() expects which here is the corpus of document to BOW.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-GRAMS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with textual data, context can be very important. As we\n",
    "discussed before, we sometimes lose this context in vector representations,\n",
    "knowing only the count of each word. N-grams, and in particular, bigrams\n",
    "are going to help us solve this problem, at least to some extent.\n",
    "An n-gram is a contiguous sequence of n items in the text. In our case, we\n",
    "will be dealing with words being the item, but depending on the use case,\n",
    "it could be even letters, syllables, or sometimes in the case of speech,\n",
    "phonemes. A bi-gram is when n = 2.\n",
    "One way bi-grams are calculated in the text is by calculating the\n",
    "conditional probability of a token given by the preceding token. It can also\n",
    "just be calculated by choosing words that appear next to each other, but it\n",
    "is more useful for us to use bi-grams that are more likely to appear as a\n",
    "pair. Such a bi-gram is called a collocation. What this means is that we're\n",
    "trying to find pairs of words that are more likely to appear around each\n",
    "other. For example, New York or Machine Learning could be two possible\n",
    "pairs of words created by bi-grams. In other words, based on the training\n",
    "data (usually the corpus), we identify that it is with high probability that\n",
    "the word York follows the word New, and that it is worth considering New\n",
    "York as one identity. We must be careful to get rid of stop words before\n",
    "running a bi-gram model on our corpus, as there could be meaningless bigrams\n",
    "formed. The Gensim bi-gram model is basically an implementation\n",
    "of collocation identification.\n",
    "We can clearly see how this is useful - we can now pick up phrases from\n",
    "our corpus, and New York certainly provides us with more information\n",
    "than the words New and York separately. This means it can be added to our\n",
    "preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "\n",
    "bigram = Phrases(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['football', 'club', 'arsenal', 'defeat', 'local', 'rival', 'weekend'],\n",
       " ['weekend', 'football', 'frenzy', 'take', 'london'],\n",
       " ['bank', 'open', 'takeover', 'bid', 'lose', 'million'],\n",
       " ['london', 'football', 'club', 'bid', 'wembley', 'stadium'],\n",
       " ['arsenal', 'bid', 'pound', 'striker', 'kane'],\n",
       " ['financial', 'trouble', 'result', 'loss', 'million', 'bank'],\n",
       " ['western', 'bank', 'file', 'bankruptcy', 'financial', 'loss'],\n",
       " ['london', 'football', 'club', 'take', 'oil', 'millionaire', 'russia'],\n",
       " ['bank', 'finance', 'work', 'russia']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [bigram[line] for line in texts]\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each line will now have all possible bi-grams created. It should be noted\n",
    "that in our toy example, we will have no bi-grams or meaningless bigrams\n",
    "being created. Since by creating new phrases we add words to our dictionary, this step\n",
    "must be done before we create our dictionary. Hence once the above step of generating bigrams is done we will then proceed to create our dictionery as done at the start of this module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**one popular preprocessing technique involves removing both high frequency and low-frequency words. We can do this in Gensim with the dictionary module. Let's say we would like to get rid of words that occur in less than 20 documents, or in more than 50% of the documents, we would add the following:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionery.filter_extremes(no_below = 20, no_above = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS TAGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the english language\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example sentences to be POS tagged\n",
    "sent_0 = nlp(u'Mathieu and I went to the park.')\n",
    "sent_1 = nlp(u'If Clement was asked to take out the garbage, he would refuse.')\n",
    "sent_2 = nlp(u'Baptiste was in charge of the refuse treatment center.')\n",
    "sent_3 = nlp(u'Marie took out her rather suspicious and fishy cat to go fish for fish.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Mathieu', 'PROPN', 'NNP')\n",
      "('and', 'CCONJ', 'CC')\n",
      "('I', 'PRON', 'PRP')\n",
      "('went', 'VERB', 'VBD')\n",
      "('to', 'ADP', 'IN')\n",
      "('the', 'DET', 'DT')\n",
      "('park', 'NOUN', 'NN')\n",
      "('.', 'PUNCT', '.')\n"
     ]
    }
   ],
   "source": [
    "for token in sent_0:\n",
    "    pprint((token.text, token.pos_, token.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('If', 'ADP', 'IN')\n",
      "('Clement', 'PROPN', 'NNP')\n",
      "('was', 'VERB', 'VBD')\n",
      "('asked', 'VERB', 'VBN')\n",
      "('to', 'PART', 'TO')\n",
      "('take', 'VERB', 'VB')\n",
      "('out', 'PART', 'RP')\n",
      "('the', 'DET', 'DT')\n",
      "('garbage', 'NOUN', 'NN')\n",
      "(',', 'PUNCT', ',')\n",
      "('he', 'PRON', 'PRP')\n",
      "('would', 'VERB', 'MD')\n",
      "('refuse', 'VERB', 'VB')\n",
      "('.', 'PUNCT', '.')\n"
     ]
    }
   ],
   "source": [
    "for token in sent_1:\n",
    "    print((token.text, token.pos_, token.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Baptiste', 'PROPN', 'NNP')\n",
      "('was', 'VERB', 'VBD')\n",
      "('in', 'ADP', 'IN')\n",
      "('charge', 'NOUN', 'NN')\n",
      "('of', 'ADP', 'IN')\n",
      "('the', 'DET', 'DT')\n",
      "('refuse', 'ADJ', 'JJ')\n",
      "('treatment', 'NOUN', 'NN')\n",
      "('center', 'NOUN', 'NN')\n",
      "('.', 'PUNCT', '.')\n"
     ]
    }
   ],
   "source": [
    "for token in sent_2:\n",
    "    print((token.text, token.pos_, token.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Harshal', 'PROPN', 'NNP')\n",
      "('is', 'VERB', 'VBZ')\n",
      "('very', 'ADV', 'RB')\n",
      "('good', 'ADJ', 'JJ')\n",
      "('at', 'ADP', 'IN')\n",
      "('NLP', 'PROPN', 'NNP')\n",
      "('.', 'PUNCT', '.')\n"
     ]
    }
   ],
   "source": [
    "sent_4 = nlp(u\"Harshal is very good at NLP.\")\n",
    "for token in sent_4:\n",
    "    print((token.text, token.pos_, token.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Marie', 'PROPN', 'NNP')\n",
      "('took', 'VERB', 'VBD')\n",
      "('out', 'PART', 'RP')\n",
      "('her', 'PRON', 'PRP')\n",
      "('rather', 'ADV', 'RB')\n",
      "('suspicious', 'ADJ', 'JJ')\n",
      "('and', 'CCONJ', 'CC')\n",
      "('fishy', 'ADJ', 'JJ')\n",
      "('cat', 'NOUN', 'NN')\n",
      "('to', 'PART', 'TO')\n",
      "('go', 'VERB', 'VB')\n",
      "('fish', 'NOUN', 'NN')\n",
      "('for', 'ADP', 'IN')\n",
      "('fish', 'NOUN', 'NN')\n",
      "('.', 'PUNCT', '.')\n"
     ]
    }
   ],
   "source": [
    "for token in sent_3:\n",
    "    print((token.text, token.pos_, token.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Mathieu\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " and I went to the park.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(sent_0, style = 'ent', jupyter = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"170-0\" class=\"displacy\" width=\"2150\" height=\"487.0\" style=\"max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">If</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">Clement</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">was</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">asked</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">to</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">take</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">out</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">garbage,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">he</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">would</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">refuse.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-170-0-0\" stroke-width=\"2px\" d=\"M70,352.0 C70,89.5 570.0,89.5 570.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-170-0-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">mark</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,354.0 L62,342.0 78,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-170-0-1\" stroke-width=\"2px\" d=\"M245,352.0 C245,177.0 565.0,177.0 565.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-170-0-1\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubjpass</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,354.0 L237,342.0 253,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-170-0-2\" stroke-width=\"2px\" d=\"M420,352.0 C420,264.5 560.0,264.5 560.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-170-0-2\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">auxpass</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,354.0 L412,342.0 428,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-170-0-3\" stroke-width=\"2px\" d=\"M595,352.0 C595,2.0 1975.0,2.0 1975.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-170-0-3\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">advcl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,354.0 L587,342.0 603,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-170-0-4\" stroke-width=\"2px\" d=\"M770,352.0 C770,264.5 910.0,264.5 910.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-170-0-4\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,354.0 L762,342.0 778,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-170-0-5\" stroke-width=\"2px\" d=\"M595,352.0 C595,177.0 915.0,177.0 915.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-170-0-5\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M915.0,354.0 L923.0,342.0 907.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-170-0-6\" stroke-width=\"2px\" d=\"M945,352.0 C945,264.5 1085.0,264.5 1085.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-170-0-6\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prt</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1085.0,354.0 L1093.0,342.0 1077.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-170-0-7\" stroke-width=\"2px\" d=\"M1295,352.0 C1295,264.5 1435.0,264.5 1435.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-170-0-7\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1295,354.0 L1287,342.0 1303,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-170-0-8\" stroke-width=\"2px\" d=\"M945,352.0 C945,89.5 1445.0,89.5 1445.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-170-0-8\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1445.0,354.0 L1453.0,342.0 1437.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-170-0-9\" stroke-width=\"2px\" d=\"M1645,352.0 C1645,177.0 1965.0,177.0 1965.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-170-0-9\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1645,354.0 L1637,342.0 1653,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-170-0-10\" stroke-width=\"2px\" d=\"M1820,352.0 C1820,264.5 1960.0,264.5 1960.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-170-0-10\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1820,354.0 L1812,342.0 1828,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(sent_1, style = 'dep', jupyter = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
