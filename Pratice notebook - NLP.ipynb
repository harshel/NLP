{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen, Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = urlopen('http://www.voidspace.org.uk').read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<', '!', 'DOCTYPE', 'html', 'PUBLIC', '``', '-//W3C//DTD', 'XHTML']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = nltk.Text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "div class=; input type=; div id=; Michael Foord; link rel=; script\n",
      "type=; Creative Commons; CSS Templates; Free CSS; open source; source\n",
      "libraries; technical blog; Python programming; script language=; img\n",
      "alt=; span id=; div style=; meta name=\n"
     ]
    }
   ],
   "source": [
    "text.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "html = urlopen(url).read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens_1 = word_tokenize(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<', '!', 'doctype', 'html', 'public', '``', '-//W3C//DTD', 'HTML', '4.0']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_1[:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = gutenberg.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = gutenberg.words(fileids = ['whitman-leaves.txt', 'carroll-alice.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'Leaves', 'of', 'Grass', 'by', 'Walt', 'Whitman', ...]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98552"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gutenberg.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2621613"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gutenberg.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = text1[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freq = FreqDist(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Harshal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word = 'How are you doing? I am doing fine, thank you for asking.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How',\n",
       " 'are',\n",
       " 'you',\n",
       " 'doing',\n",
       " '?',\n",
       " 'I',\n",
       " 'am',\n",
       " 'doing',\n",
       " 'fine',\n",
       " ',',\n",
       " 'thank',\n",
       " 'you',\n",
       " 'for',\n",
       " 'asking',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GETTING THE SYNONYMS OF A WORD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_1 = wordnet.synsets('spectacular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('spectacular.n.01'),\n",
       " Synset('dramatic.s.02'),\n",
       " Synset('spectacular.s.02'),\n",
       " Synset('outstanding.s.02')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DRAMATIC meaning:  sensational in appearance or thrilling in effect\n"
     ]
    }
   ],
   "source": [
    "print('DRAMATIC meaning: ', word_1[1].definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "having a quality that thrusts itself into attention\n"
     ]
    }
   ],
   "source": [
    "print(word_1[3].definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEMMING AND LEMATIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEMMING**: Removes the affixes from a word and returns a root word (which may not be real word).\n",
    "\n",
    "**LEMMATIZATION** : Similar to stemming but returns an actual word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer #sometimes does not return an actual word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stem_word = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefix\n"
     ]
    }
   ],
   "source": [
    "print(stem_word.stem('Prefixes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differ\n"
     ]
    }
   ],
   "source": [
    "print(stem_word.stem('differences'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decres\n"
     ]
    }
   ],
   "source": [
    "print(stem_word.stem('decreses'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer #returns an actual word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decrease\n"
     ]
    }
   ],
   "source": [
    "print(lemma.lemmatize('decreases'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference\n"
     ]
    }
   ],
   "source": [
    "print(lemma.lemmatize('differences'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happier\n"
     ]
    }
   ],
   "source": [
    "print(lemma.lemmatize('happier'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEXTBLOB for sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Polarity** : Defines negativity or positivity in the sentence, whereas\n",
    "\n",
    "**Subjectivity** : Implies whether the sentence discusses something vaguely or with complete surety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = TextBlob('My school is far away from my home.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.1, subjectivity=1.0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text1 = TextBlob('I am not good at maths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=-0.35, subjectivity=0.6000000000000001)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text2 = TextBlob('I am very good at solving maths problem.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.9099999999999999, subjectivity=0.7800000000000001)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text3 = TextBlob(\"I don't know what i am doing here.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.0, subjectivity=0.0)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text3.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Harshal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Harshal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = '''How about you and I go together on a walk far away\n",
    "from this place, discussing the things we have never discussed\n",
    "on Deep Learning and Natural Language Processing.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blob = TextBlob(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('How', 'WRB'),\n",
       " ('about', 'IN'),\n",
       " ('you', 'PRP'),\n",
       " ('and', 'CC'),\n",
       " ('I', 'PRP'),\n",
       " ('go', 'VBP'),\n",
       " ('together', 'RB'),\n",
       " ('on', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('walk', 'NN'),\n",
       " ('far', 'RB'),\n",
       " ('away', 'RB'),\n",
       " ('from', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('place', 'NN'),\n",
       " ('discussing', 'VBG'),\n",
       " ('the', 'DT'),\n",
       " ('things', 'NNS'),\n",
       " ('we', 'PRP'),\n",
       " ('have', 'VBP'),\n",
       " ('never', 'RB'),\n",
       " ('discussed', 'VBN'),\n",
       " ('on', 'IN'),\n",
       " ('Deep', 'NNP'),\n",
       " ('Learning', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('Natural', 'NNP'),\n",
       " ('Language', 'NNP'),\n",
       " ('Processing', 'NNP')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.06666666666666667, subjectivity=0.6)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#textblob can be used to correct the misspelled words\n",
    "text = 'I have no idae of whta is goign on in here'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blob_correct = TextBlob(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"I have no idea of what is going on in here\")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob_correct.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#textblob can be used for doing language translation\n",
    "lang = TextBlob('Salut, quel est ton nom?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Hi, what's your name?\")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang.translate(from_lang = 'fr', to = 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lang1 = TextBlob(\"pouvez-vous me dire ce qui se passe ici?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"can you tell me what's going on here?\")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang1.translate(from_lang = 'fr', to = 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lang2 = TextBlob(\"L'apprentissage automatique a conféré un pouvoir incroyable aux humains. Le pouvoir d'exécuter des tâches de manière automatisée, le pouvoir de rendre nos vies confortables, le pouvoir d'améliorer continuellement les choses en étudiant les décisions à grande échelle et le pouvoir de créer des espèces qui pensent mieux que les humains. Cette liste peut continuer encore et encore.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Machine learning has brought incredible power to humans. The power to perform tasks in an automated way, the power to make our lives comfortable, the power to continually improve things by studying large-scale decisions and the power to create species that think better than humans. This list can continue over and over again.\")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang2.translate(from_lang = 'fr', to = 'en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SPACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "william = \"\"\"William was the son of King William\n",
    "II and Anna Pavlovna of Russia. On the abdication of his\n",
    "grandfather William I in 1840, he became the Prince of Orange.\n",
    "On the death of his father in 1849, he succeeded as king of the\n",
    "Netherlands. William married his cousin Sophie of Württemberg\n",
    "in 1839 and they had three sons, William, Maurice, and\n",
    "Alexander, all of whom predeceased him. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = nlp(william)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(William, William\n",
       " II, Anna Pavlovna, Russia, , William I, 1840, the Prince of Orange, , 1849, , Netherlands, William, Sophie, Württemberg, 1839, three, William, Maurice, , Alexander)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents #Named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(William, 'PERSON', 378), (William\n",
      "II, 'PERSON', 378), (Anna Pavlovna, 'PERSON', 378), (Russia, 'GPE', 382), (\n",
      ", 'GPE', 382), (William I, 'PERSON', 378), (1840, 'DATE', 388), (the Prince of Orange, 'LOC', 383), (\n",
      ", 'GPE', 382), (1849, 'DATE', 388), (\n",
      ", 'GPE', 382), (Netherlands, 'GPE', 382), (William, 'PERSON', 378), (Sophie, 'PERSON', 378), (Württemberg\n",
      ", 'PERSON', 378), (1839, 'DATE', 388), (three, 'CARDINAL', 394), (William, 'PERSON', 378), (Maurice, 'PERSON', 378), (\n",
      ", 'GPE', 382), (Alexander, 'ORG', 381)]\n"
     ]
    }
   ],
   "source": [
    "print([(i, i.label_, i.label) for i in doc.ents]) #named entity recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GENSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = \"must read books for beginners on machine learning and artificial intelligence analytics vidhya\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = words.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['must',\n",
       " 'read',\n",
       " 'books',\n",
       " 'for',\n",
       " 'beginners',\n",
       " 'on',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'and',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'analytics',\n",
       " 'vidhya']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(sentence, size = 50, min_count = 0, window = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harshal\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.00842949, -0.00618888, -0.00560426,  0.00915599, -0.00229086,\n",
       "       -0.00851648, -0.00847657,  0.00096461, -0.00365682,  0.00351385,\n",
       "        0.00626107, -0.00431012,  0.0096262 ,  0.00411544, -0.00202899,\n",
       "       -0.00686036, -0.00126503,  0.00889008, -0.00319603, -0.00839249,\n",
       "        0.00444005,  0.00572109, -0.00784398, -0.00873718,  0.00435577,\n",
       "        0.00036335,  0.00578291, -0.00324825,  0.00480447, -0.00649574,\n",
       "        0.00459446,  0.00981119,  0.00254486, -0.00113519, -0.0015491 ,\n",
       "        0.00749547,  0.00239623, -0.00308201, -0.0029779 ,  0.00224804,\n",
       "       -0.00340135,  0.00282469,  0.0086577 ,  0.00405177, -0.00342716,\n",
       "        0.00600996,  0.00147674,  0.00926899,  0.00356444,  0.00629241],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['a']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEXT SEARCH USING REGULAR EXPRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = ['very', 'nice', 'lecture', 'day', 'noon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = '|'.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'very|nice|lecture|day|noon'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['very', 'nice', 'lecture']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(words, 'i attended a very nice lecture last year', re.M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEXT TO LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# individual words as a separate elements of the list\n",
    "with open ('text.txt') as f:\n",
    "    words = f.read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'book', 'is', 'written', 'by', 'Yaser', 'Abu', 'Mostafa,', 'Malik', 'Magdon-Ismail', 'and', 'Hsuan-Tien', 'Lin.', 'It', 'provides', 'a', 'perfect', 'introduction', 'to', 'machine', 'learning.', 'This', 'book', 'prepares', 'you', 'to', 'understand', 'complex', 'areas', 'of', 'machine', 'learning.', 'Yaser,', 'a', 'very', 'popular', 'and', 'brilliant', 'professor,', 'has', 'provided', '‘to', 'the', 'point’', 'explanations', 'instead', 'of', 'lengthy', 'and', 'go-around', 'explanations.', 'If', 'you', 'choose', 'this', 'book,', 'I’d', 'suggest', 'you', 'to', 'refer', 'to', 'online', 'tutorials', 'of', 'Yaser', 'Abu', 'Mostafa', 'as', 'well.', 'They’re', 'awesome.']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#whole text as a single element of the  list\n",
    "with open('text.txt', 'r') as f:\n",
    "    words_= f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This book is written by Yaser Abu Mostafa, Malik Magdon-Ismail and Hsuan-Tien Lin. It provides a perfect\\n', 'introduction to machine learning. This book prepares you to understand complex areas of machine learning. Yaser, a very popular and brilliant professor, has provided ‘to the point’ explanations \\n', 'instead of lengthy and go-around explanations. If you choose this book, I’d suggest you to refer to \\n', 'online tutorials of Yaser Abu Mostafa as well. They’re awesome.']\n"
     ]
    }
   ],
   "source": [
    "print(words_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PREPROCESSING THE TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = \"John has been selected for the trial phase this time. Congrats!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#converting all the words to lower\n",
    "sentence = sentence.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining the positive and negative words explicitly\n",
    "positive_words=['awesome','good', 'nice', 'super', 'fun', 'delightful', 'congrats']\n",
    "negative_words=['awful','lame','horrible','bad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove all the elements of less importance\n",
    "sentence = sentence.replace('!', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split the sentence into individual words\n",
    "sentence = sentence.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['john', 'has', 'been', 'selected', 'for', 'the', 'trial', 'phase', 'this', 'time.', 'congrats']\n"
     ]
    }
   ],
   "source": [
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the', 'phase', 'selected', 'trial', 'been', 'this', 'has', 'time.', 'john', 'for', 'congrats'}\n",
      "{'awesome', 'good', 'nice', 'super', 'fun', 'congrats', 'delightful'}\n"
     ]
    }
   ],
   "source": [
    "print(set(sentence))\n",
    "print(set(positive_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#removing the positive word from the sentence\n",
    "result = set(sentence) - set(positive_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the', 'phase', 'selected', 'trial', 'been', 'this', 'has', 'time.', 'john', 'for'}\n"
     ]
    }
   ],
   "source": [
    "print(result)  #the word 'congrats' is been removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REMOVING THE STOP WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = \"This book is about Deep Learning and Natural Language Processing!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'book', 'is', 'about', 'Deep', 'Learning', 'and', 'Natural', 'Language', 'Processing', '!']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords_remove = [w for w in words if w not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'book', 'Deep', 'Learning', 'Natural', 'Language', 'Processing', '!']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COUNT VECTORIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = ['The quick brown fox jumped over the lazy dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the instance of the class\n",
    "vectorize = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize and build the vocab\n",
    "vectorize.fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'brown': 0,\n",
       " 'dog': 1,\n",
       " 'fox': 2,\n",
       " 'jumped': 3,\n",
       " 'lazy': 4,\n",
       " 'over': 5,\n",
       " 'quick': 6,\n",
       " 'the': 7}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summarize\n",
    "vectorize.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector = vectorize.transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8)\n",
      "[[1 1 1 1 1 1 1 2]]\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(vector.shape)\n",
    "print(vector.toarray())\n",
    "print(type(vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then see that the encoded vector is a sparse matrix. Finally, we can see an array version of the encoded vector showing a count of 1 occurrence for each word except the (index and id 7) that has an occurrence of 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, the same vectorizer can be used on documents that contain words not included in the vocabulary. These words are ignored and no count is given in the resulting vector.\n",
    "For example, below is an example of using the vectorizer above to encode a document with one word in the vocab and one word that is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text2 = ['the puppy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector1 = vectorize.transform(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8)\n",
      "[[0 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(vector1.shape)\n",
    "print(vector1.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this example prints the array version of the encoded sparse vector showing one occurrence of the one word in the vocab and the other word not in the vocab completely ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF SCORE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF is an acronym of two terms: term frequency and inverse document\n",
    "frequency. TF is the ratio representing the count of specific words to the\n",
    "total number of words in a document. Suppose that a document contains\n",
    "100 words, wherein the word happy appears five times. The term frequency\n",
    "(i.e., tf ) for happy is then (5/100) = 0.05. IDF, on the other hand, is a\n",
    "log ratio of the total number of documents to a document containing a\n",
    "particular word. Suppose we have 10 million documents, and the word\n",
    "happy appears in 1,000 of them. The inverse document frequency (i.e., idf ),\n",
    "then, would be calculated as log (10,000,000/1,000) = 4. Thus, the TF-IDF\n",
    "weight is the product of these quantities: 0.05 × 4 = 0.20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts=[\"Ramiess sings classic songs\",\"he listens to old pop\",\"and rock music\", ' and also listens to classical songs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create the class of tfidf vectorizer\n",
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.fit([texts[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ramiess': 1, 'sings': 2, 'classic': 0, 'songs': 3}\n"
     ]
    }
   ],
   "source": [
    "# summarize\n",
    "print(tfidf.vocabulary_) #each word is assigned a unique integer index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#encode the document\n",
    "vector_tfidf = tfidf.transform([texts[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4)\n",
      "[[0.5 0.5 0.5 0.5]]\n"
     ]
    }
   ],
   "source": [
    "print(vector_tfidf.shape)\n",
    "print(vector_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEXT CLASSIFIER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text can be classified into various classes, such as positive and negative.\n",
    "TextBlob offers many such architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob.classifiers import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = [('I love my country.', 'pos'),\n",
    "('This is an amazing place!', 'pos'),\n",
    "('I do not like the smell of this place.', 'neg'),\n",
    "('I do not like this restaurant', 'neg'),\n",
    "('I am tired of hearing your nonsense.', 'neg'),\n",
    "(\"I always aspire to be like him\", 'pos'),\n",
    "(\"It's a horrible performance.\", \"neg\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#applying the naive bayes classfier on the input text\n",
    "model = NaiveBayesClassifier(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classify('This is s good place')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classify('i hate this place')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classify('You are a fucking nerd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classify('You are such a neerd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classify('you are a nerd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 25 - 10 - 2018\n",
    "# BOOK - ARTIFICIAL INTELLIGENCE WITH PYTHON BY PRATEEK JOSHI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOKENIZING THE TEXT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define input text\n",
    "input_text = \"Do you know how tokenization works? It's actually quite interesting! \\\n",
    "Let's analyze a couple of sentences and figure it out.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do you know how tokenization works?', \"It's actually quite interesting!\", \"Let's analyze a couple of sentences and figure it out.\"]\n"
     ]
    }
   ],
   "source": [
    "# split into sentences\n",
    "print(sent_tokenize(input_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', 'you', 'know', 'how', 'tokenization', 'works', '?', 'It', \"'s\", 'actually', 'quite', 'interesting', '!', 'Let', \"'s\", 'analyze', 'a', 'couple', 'of', 'sentences', 'and', 'figure', 'it', 'out', '.']\n"
     ]
    }
   ],
   "source": [
    "# split into individual words\n",
    "print(word_tokenize(input_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', 'you', 'know', 'how', 'tokenization', 'works', '?', 'It', \"'\", 's', 'actually', 'quite', 'interesting', '!', 'Let', \"'\", 's', 'analyze', 'a', 'couple', 'of', 'sentences', 'and', 'figure', 'it', 'out', '.']\n"
     ]
    }
   ],
   "source": [
    "# split into individual words\n",
    "print(WordPunctTokenizer().tokenize(input_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the diffence between the word_tokenize and WordPunctTokenizer can be seen when tokenizing the punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONVERTING WORDS TO THEIR BASE FORM\n",
    "1. Stemming\n",
    "2. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_words = ['writing', 'calves', 'be', 'branded','horse', 'randomize','possibly', 'provision', 'hospital', 'kept',\\\n",
    "               'scratchy', 'code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "       INPUT WORD          PORTER       LANCASTER        SNOWBALL \n",
      " ====================================================================\n"
     ]
    }
   ],
   "source": [
    "# create the object of each class\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "       INPUT WORD          PORTER       LANCASTER        SNOWBALL \n",
      " ====================================================================\n",
      "         writing           write            writ           write\n",
      "          calves            calv            calv            calv\n",
      "              be              be              be              be\n",
      "         branded           brand           brand           brand\n",
      "           horse            hors            hors            hors\n",
      "       randomize          random          random          random\n",
      "        possibly         possibl            poss         possibl\n",
      "       provision          provis          provid          provis\n",
      "        hospital          hospit          hospit          hospit\n",
      "            kept            kept            kept            kept\n",
      "        scratchy        scratchi        scratchy        scratchi\n",
      "            code            code             cod            code\n"
     ]
    }
   ],
   "source": [
    "stemmer = ['PORTER', 'LANCASTER', 'SNOWBALL']\n",
    "formatted_text = '{:>16}' * (len(stemmer) + 1)\n",
    "print('\\n', formatted_text.format('INPUT WORD',*stemmer), '\\n', '='*68)\n",
    "\n",
    "for words in input_words:\n",
    "    output = (words, porter.stem(words), lancaster.stem(words), snowball.stem(words))\n",
    "    print(formatted_text.format(*output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between the three stemming algorithms is the level of strictness that is being used to arrive at the root word.\n",
    "\n",
    "**LANCASTER** algorithm seems to be more strict as it cuts the maximum characters from a word.\n",
    "\n",
    "**PORTER** algorithm is the least strict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEMMATIZATION\n",
    "\n",
    "Lemmatization is another way of reducing a word to it's base form. Use of Stemming algorithms to reduce a word to it's base form does not always provide an output having a proper meaning. For example, the word 'calves' is reduced to 'calv' which is not a word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_words = ['writing', 'calves', 'be', 'branded','horse', 'randomize','possibly', 'provision', 'hospital', \\\n",
    "               'kept','scratchy', 'code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "               INPUT WORD         NOUN LEMMATIZER         VERB LEMMATIZER \n",
      " ===========================================================================\n",
      "                 writing                 writing                   write\n",
      "                  calves                    calf                   calve\n",
      "                      be                      be                      be\n",
      "                 branded                 branded                   brand\n",
      "                   horse                   horse                   horse\n",
      "               randomize               randomize               randomize\n",
      "                possibly                possibly                possibly\n",
      "               provision               provision               provision\n",
      "                hospital                hospital                hospital\n",
      "                    kept                    kept                    keep\n",
      "                scratchy                scratchy                scratchy\n",
      "                    code                    code                    code\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer_names = ['NOUN LEMMATIZER', 'VERB LEMMATIZER']\n",
    "formatted_text = '{:>24}' * (len(lemmatizer_names) +1)\n",
    "print('\\n', formatted_text.format('INPUT WORD',*lemmatizer_names),'\\n', '='*75)\n",
    "for words in input_words:\n",
    "    output = [words, lemmatizer.lemmatize(words, pos='n'), lemmatizer.lemmatize(words, pos='v')]\n",
    "    print(formatted_text.format(*output))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 1 ==> The Fulton County Grand Jury said Friday an invest\n",
      "chunk 2 ==> '' . ( 2 ) Fulton legislators `` work with city of\n",
      "chunk 3 ==> . Construction bonds Meanwhile , it was learned th\n",
      "chunk 4 ==> , anonymous midnight phone calls and veiled threat\n",
      "chunk 5 ==> Harris , Bexar , Tarrant and El Paso would be $451\n",
      "chunk 6 ==> set it for public hearing on Feb. 22 . The proposa\n",
      "chunk 7 ==> College . He has served as a border patrolman and \n",
      "chunk 8 ==> of his staff were doing on the address involved co\n",
      "chunk 9 ==> plan alone would boost the base to $5,000 a year a\n",
      "chunk 10 ==> nursing homes In the area of `` community health s\n",
      "chunk 11 ==> of its Angola policy prove harsh , there has been \n",
      "chunk 12 ==> system which will prevent Laos from being used as \n",
      "chunk 13 ==> reform in recipient nations . In Laos , the admini\n",
      "chunk 14 ==> . He is not interested in being named a full-time \n",
      "chunk 15 ==> said , `` to obtain the views of the general publi\n",
      "chunk 16 ==> '' . Mr. Reama , far from really being retired , i\n",
      "chunk 17 ==> making enforcement of minor offenses more effectiv\n",
      "chunk 18 ==> to tell the people where he stands on the tax issu\n"
     ]
    }
   ],
   "source": [
    "# define a function for chunking\n",
    "def chunker(input_data, N):\n",
    "    \"\"\"input_words = Data given as input\n",
    "    N = Number of words in each chunk\"\"\"\n",
    "    input_words = input_data.split(' ')\n",
    "    output = []\n",
    "    \n",
    "    chunks = []\n",
    "    chunk_count = 0\n",
    "    for words in input_words:\n",
    "        chunks.append(words)\n",
    "        chunk_count += 1\n",
    "        \n",
    "        if chunk_count == N:\n",
    "            output.append(' '.join(chunks))\n",
    "            chunk_count, chunks = 0, []\n",
    "    output.append(' '.join(chunks))\n",
    "    return output\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_data = ' '.join(brown.words()[:12000])\n",
    "    N_chunk = 700\n",
    "\n",
    "    chunk = chunker(input_data = input_data, N = N_chunk)\n",
    "    for i, word in enumerate(chunk):\n",
    "        print('chunk', i+1, '==>', word[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
